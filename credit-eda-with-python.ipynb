{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit EDA With Python\n\nLet's explore credit data present in this [link](https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/develop/dataset/credito.csv). The data is in CSV format and contains information about a financial institution's customers.","metadata":{}},{"cell_type":"markdown","source":"# 1. Problem Overview\n\nOur primary focus lies on elucidating the intricacies of the second column, denoted as default. This column serves as a binary indicator, distinguishing customers who are not in default (`default = 0`) from those in default (`default = 1`). \n\nOur aim is to discern the factors contributing to a client's failure to meet financial obligations, drawing insights from attributes such as salary, education, and financial transactions.\n\n> In this context, the attribute of interest, namely default, assumes the role of the **response variable** or **dependent variable**, while attributes like (`age`, `salary`, etc.), function as **explanatory variables**, independent variables, or even predictor variables.","metadata":{}},{"cell_type":"markdown","source":"| Coluna  | Descrição |\n| ------- | --------- |\n| id      | Número da conta |\n| default | Indica se o cliente é adimplente (0) ou inadimplente (1) |\n| idade   | --- |\n| sexo    | --- |\n| depedentes | --- |\n| escolaridade | --- |\n| estado_civil | --- |\n| salario_anual | Faixa do salario mensal multiplicado por 12 |\n| tipo_cartao | Categoria do cartao: blue, silver, gold e platinium |\n| meses_de_relacionamento | Quantidade de meses desde a abertura da conta |\n| qtd_produtos | Quantidade de produtos contratados |\n| iteracoes_12m | Quantidade de iteracoes com o cliente no último ano |\n| meses_inatico_12m | Quantidade de meses que o cliente ficou inativo no último ano |\n| limite_credito | Valor do limite do cartão de crédito |\n| valor_transacoes_12m | Soma total do valor das transações no cartão de crédito no último ano |\n| qtd_transacoes_12m | Quantidade total de transações no cartão de crédito no último ano |","metadata":{}},{"cell_type":"markdown","source":"# 2. Importing Libraries\n\nIn this step, we ensure we have the essential tools by importing the necessary libraries. This includes leveraging popular libraries like Pandas, NumPy, and Matplotlib, which empower us to efficiently manipulate data and create insightful visualizations. This foundational setup establishes a robust framework for our data exploration and analysis.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nimport sklearn","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:57.334035Z","iopub.execute_input":"2023-11-23T02:34:57.334775Z","iopub.status.idle":"2023-11-23T02:34:57.341560Z","shell.execute_reply.started":"2023-11-23T02:34:57.334718Z","shell.execute_reply":"2023-11-23T02:34:57.340533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Dataset - Loading/Viewing/Generation\n\nOur journey begins with the foundational step of loading data into a `Pandas` dataframe:","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/develop/dataset/credito.csv', na_values='na')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:57.343306Z","iopub.execute_input":"2023-11-23T02:34:57.343867Z","iopub.status.idle":"2023-11-23T02:34:57.939051Z","shell.execute_reply.started":"2023-11-23T02:34:57.343833Z","shell.execute_reply":"2023-11-23T02:34:57.938211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(n=10)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:57.940433Z","iopub.execute_input":"2023-11-23T02:34:57.940970Z","iopub.status.idle":"2023-11-23T02:34:57.961617Z","shell.execute_reply.started":"2023-11-23T02:34:57.940939Z","shell.execute_reply":"2023-11-23T02:34:57.960627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Exploration stage\n\nNow that we have our dataset loaded, it's time to delve into its structure and relationships. We'll employ a correlation matrix, visually represented by a heatmap.\n\nThis exploration step lays the groundwork for uncovering patterns and gaining valuable insights from our data.","metadata":{}},{"cell_type":"code","source":"# Filter numeric columns for correlation analysis\nnumeric_columns = df.select_dtypes(include=['number']).columns\ncorr_matrix = df[numeric_columns].corr()\n\n# Identify highly correlated variables (correlation > 0.75)\nhigh_corr_var = []\nfor col in corr_matrix:\n    highly_corr_rows = corr_matrix[col][corr_matrix[col] > 0.75].index\n    high_corr_var.extend([[col, row, corr_matrix[col][row]] for row in highly_corr_rows if row != col])\n\n# Display the highly correlated variables\nprint(high_corr_var)\n\n# Drop one of the redundant correlated variables\nif high_corr_var:\n    df.drop(high_corr_var[0][1], axis=1, inplace=True)\n\n# Visualize the correlation matrix using a heatmap\nplt.figure(figsize=(10, 8))\nheatmap = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n\n# Highlight the highly correlated variables on the heatmap\nfor var in high_corr_var:\n    correlation_value = corr_matrix.loc[var[0], var[1]]\n    heatmap.text(\n        numeric_columns.get_loc(var[1]) + 0.5,  # Adjusted the column index\n        numeric_columns.get_loc(var[0]) + 0.5,  # Adjusted the row index\n        f\"{var[0]} & {var[1]} ({correlation_value:.2f})\",\n        color='red',\n        ha='center',\n        va='center',\n        fontsize=10,\n        rotation=45  # Rotated the text for better visibility\n    )\n\n# Display the title and heatmap\nplt.title('Correlation Matrix')\n\n# Remove red text from the graph\nplt.text(0, 0, '', color='red')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:42:29.287059Z","iopub.execute_input":"2023-11-23T02:42:29.287548Z","iopub.status.idle":"2023-11-23T02:42:30.106227Z","shell.execute_reply.started":"2023-11-23T02:42:29.287512Z","shell.execute_reply":"2023-11-23T02:42:30.104685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape # retorna uma tupla (qtd linhas, qtd colunas)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:58.772534Z","iopub.execute_input":"2023-11-23T02:34:58.773248Z","iopub.status.idle":"2023-11-23T02:34:58.784372Z","shell.execute_reply.started":"2023-11-23T02:34:58.773201Z","shell.execute_reply":"2023-11-23T02:34:58.782150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['default'] == 0].shape","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:58.786468Z","iopub.execute_input":"2023-11-23T02:34:58.787001Z","iopub.status.idle":"2023-11-23T02:34:58.799987Z","shell.execute_reply.started":"2023-11-23T02:34:58.786961Z","shell.execute_reply":"2023-11-23T02:34:58.798456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['default'] == 1].shape","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:58.802150Z","iopub.execute_input":"2023-11-23T02:34:58.802638Z","iopub.status.idle":"2023-11-23T02:34:58.815177Z","shell.execute_reply.started":"2023-11-23T02:34:58.802592Z","shell.execute_reply":"2023-11-23T02:34:58.813440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qtd_total, _ = df.shape\nqtd_adimplentes, _ = df[df['default'] == 0].shape\nqtd_inadimplentes, _ = df[df['default'] == 1].shape","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:58.817018Z","iopub.execute_input":"2023-11-23T02:34:58.817555Z","iopub.status.idle":"2023-11-23T02:34:58.830683Z","shell.execute_reply.started":"2023-11-23T02:34:58.817505Z","shell.execute_reply":"2023-11-23T02:34:58.829228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The proportion of paying customers is {round(100 * qtd_adimplentes / qtd_total, 2)}%\")\nprint(f\"The proportion of defaulting customers is {round(100 * qtd_inadimplentes / qtd_total, 2)}%\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:58.832903Z","iopub.execute_input":"2023-11-23T02:34:58.833398Z","iopub.status.idle":"2023-11-23T02:34:58.841651Z","shell.execute_reply.started":"2023-11-23T02:34:58.833336Z","shell.execute_reply":"2023-11-23T02:34:58.840032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Cleaning and Transformation\n\nFirst lets see the data types and its attributes So then we can make the necessary adjustments regarding data standardization and cleaning","metadata":{}},{"cell_type":"code","source":"df.head(n=5)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:58.847297Z","iopub.execute_input":"2023-11-23T02:34:58.847719Z","iopub.status.idle":"2023-11-23T02:34:58.869113Z","shell.execute_reply.started":"2023-11-23T02:34:58.847687Z","shell.execute_reply":"2023-11-23T02:34:58.867787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Columns** and their respective **data types**.","metadata":{}},{"cell_type":"code","source":"# Display data types of each column\nprint(df.dtypes)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:58.870630Z","iopub.execute_input":"2023-11-23T02:34:58.871161Z","iopub.status.idle":"2023-11-23T02:34:58.880833Z","shell.execute_reply.started":"2023-11-23T02:34:58.871026Z","shell.execute_reply":"2023-11-23T02:34:58.879277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Categorical** attributes.","metadata":{}},{"cell_type":"code","source":"df.select_dtypes('object').describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:58.882240Z","iopub.execute_input":"2023-11-23T02:34:58.883636Z","iopub.status.idle":"2023-11-23T02:34:58.944979Z","shell.execute_reply.started":"2023-11-23T02:34:58.883569Z","shell.execute_reply":"2023-11-23T02:34:58.943730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Numeric** attributes.","metadata":{}},{"cell_type":"code","source":"df.drop('id', axis=1).select_dtypes('number').describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:58.946635Z","iopub.execute_input":"2023-11-23T02:34:58.947882Z","iopub.status.idle":"2023-11-23T02:34:58.997720Z","shell.execute_reply.started":"2023-11-23T02:34:58.947841Z","shell.execute_reply":"2023-11-23T02:34:58.996251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Missing data could be:\n\n  - Empty (`\"\"`);\n  - Nulls (`None`);\n  - Not available or applicable (`na`, `NA`, etc.);\n  - Non-numeric (`nan`, `NaN`, `NAN`, etc).","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:58.999225Z","iopub.execute_input":"2023-11-23T02:34:58.999666Z","iopub.status.idle":"2023-11-23T02:34:59.020853Z","shell.execute_reply.started":"2023-11-23T02:34:58.999630Z","shell.execute_reply":"2023-11-23T02:34:59.019518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can check which columns have missing data.","metadata":{}},{"cell_type":"code","source":"df.isna().any()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.022740Z","iopub.execute_input":"2023-11-23T02:34:59.023785Z","iopub.status.idle":"2023-11-23T02:34:59.041458Z","shell.execute_reply.started":"2023-11-23T02:34:59.023744Z","shell.execute_reply":"2023-11-23T02:34:59.040168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The function below generates some statistics on the missing data columns.","metadata":{}},{"cell_type":"code","source":"def stats_dados_faltantes(df: pd.DataFrame) -> None:\n\n  stats_dados_faltantes = []\n  for col in df.columns:\n    if df[col].isna().any():\n      qtd, _ = df[df[col].isna()].shape\n      total, _ = df.shape\n      dict_dados_faltantes = {col: {'quantidade': qtd, \"porcentagem\": round(100 * qtd/total, 2)}}\n      stats_dados_faltantes.append(dict_dados_faltantes)\n\n  for stat in stats_dados_faltantes:\n    print(stat)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.042788Z","iopub.execute_input":"2023-11-23T02:34:59.043119Z","iopub.status.idle":"2023-11-23T02:34:59.054077Z","shell.execute_reply.started":"2023-11-23T02:34:59.043090Z","shell.execute_reply":"2023-11-23T02:34:59.052788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stats_dados_faltantes(df=df)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.056212Z","iopub.execute_input":"2023-11-23T02:34:59.057309Z","iopub.status.idle":"2023-11-23T02:34:59.089695Z","shell.execute_reply.started":"2023-11-23T02:34:59.057251Z","shell.execute_reply":"2023-11-23T02:34:59.088379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stats_dados_faltantes(df=df[df['default'] == 0])","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.092529Z","iopub.execute_input":"2023-11-23T02:34:59.092958Z","iopub.status.idle":"2023-11-23T02:34:59.118295Z","shell.execute_reply.started":"2023-11-23T02:34:59.092926Z","shell.execute_reply":"2023-11-23T02:34:59.117136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stats_dados_faltantes(df=df[df['default'] == 1])","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.120254Z","iopub.execute_input":"2023-11-23T02:34:59.120993Z","iopub.status.idle":"2023-11-23T02:34:59.135526Z","shell.execute_reply.started":"2023-11-23T02:34:59.120948Z","shell.execute_reply":"2023-11-23T02:34:59.134435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**# Data Wrangling**\n\nNow that we have gained a deeper understanding of our dataset, it's time to perform data wrangling, a crucial step in preparing the data for analysis. Our focus will be on:\n\n  - Correcting the column schema for better interpretation;\n  - Handling missing data effectively.\n\nDuring the exploration stage, we observed that the columns 'limite_credito' and 'valor_transacoes_12m' were incorrectly interpreted as categorical columns (`dtype = object`).\n\nLet's proceed with the necessary transformations.","metadata":{}},{"cell_type":"code","source":"df[['limite_credito', 'valor_transacoes_12m']].dtypes","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.136768Z","iopub.execute_input":"2023-11-23T02:34:59.137112Z","iopub.status.idle":"2023-11-23T02:34:59.151741Z","shell.execute_reply.started":"2023-11-23T02:34:59.137082Z","shell.execute_reply":"2023-11-23T02:34:59.150896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['limite_credito', 'valor_transacoes_12m']].head(n=5)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.153243Z","iopub.execute_input":"2023-11-23T02:34:59.155504Z","iopub.status.idle":"2023-11-23T02:34:59.174097Z","shell.execute_reply.started":"2023-11-23T02:34:59.155431Z","shell.execute_reply":"2023-11-23T02:34:59.172925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we'll implement a `lambda` function for data cleaning. Before applying this function, let's conduct a preliminary test using the `map` functional method to ensure its effectiveness:","metadata":{}},{"cell_type":"code","source":"fn = lambda valor: float(valor.replace(\".\", \"\").replace(\",\", \".\"))\n\nvalores_originais = ['12.691,51', '8.256,96', '3.418,56', '3.313,03', '4.716,22']\nvalores_limpos = list(map(fn, valores_originais))\n\nprint(valores_originais)\nprint(valores_limpos)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.175832Z","iopub.execute_input":"2023-11-23T02:34:59.177025Z","iopub.status.idle":"2023-11-23T02:34:59.185534Z","shell.execute_reply.started":"2023-11-23T02:34:59.176976Z","shell.execute_reply":"2023-11-23T02:34:59.184414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that our `lambda` cleaning function is prepared, effortlessly apply it to the selected columns for a seamless data transformation.","metadata":{}},{"cell_type":"code","source":"df['valor_transacoes_12m'] = df['valor_transacoes_12m'].apply(lambda x: fn(x) if isinstance(x, str) else x)\ndf['limite_credito'] = df['limite_credito'].apply(lambda x: fn(x) if isinstance(x, str) else x)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.186775Z","iopub.execute_input":"2023-11-23T02:34:59.187161Z","iopub.status.idle":"2023-11-23T02:34:59.226252Z","shell.execute_reply.started":"2023-11-23T02:34:59.187130Z","shell.execute_reply":"2023-11-23T02:34:59.224836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's revisit the description of the dataset structure:","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.227743Z","iopub.execute_input":"2023-11-23T02:34:59.229437Z","iopub.status.idle":"2023-11-23T02:34:59.238660Z","shell.execute_reply.started":"2023-11-23T02:34:59.229391Z","shell.execute_reply":"2023-11-23T02:34:59.237466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Categorical** attributes.","metadata":{}},{"cell_type":"code","source":"df.select_dtypes('object').describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.240388Z","iopub.execute_input":"2023-11-23T02:34:59.241760Z","iopub.status.idle":"2023-11-23T02:34:59.280978Z","shell.execute_reply.started":"2023-11-23T02:34:59.241705Z","shell.execute_reply":"2023-11-23T02:34:59.279735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Numeric** attributes.","metadata":{}},{"cell_type":"code","source":"df.drop('id', axis=1).select_dtypes('number').describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.282699Z","iopub.execute_input":"2023-11-23T02:34:59.283203Z","iopub.status.idle":"2023-11-23T02:34:59.334816Z","shell.execute_reply.started":"2023-11-23T02:34:59.283157Z","shell.execute_reply":"2023-11-23T02:34:59.333432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since pandas is aware of missing data, removing the problematic rows is trivial.","metadata":{}},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.336565Z","iopub.execute_input":"2023-11-23T02:34:59.336932Z","iopub.status.idle":"2023-11-23T02:34:59.350803Z","shell.execute_reply.started":"2023-11-23T02:34:59.336902Z","shell.execute_reply":"2023-11-23T02:34:59.349754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the data structure again.","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.357536Z","iopub.execute_input":"2023-11-23T02:34:59.358231Z","iopub.status.idle":"2023-11-23T02:34:59.366589Z","shell.execute_reply.started":"2023-11-23T02:34:59.358195Z","shell.execute_reply":"2023-11-23T02:34:59.365458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['default'] == 0].shape","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.368147Z","iopub.execute_input":"2023-11-23T02:34:59.369231Z","iopub.status.idle":"2023-11-23T02:34:59.380758Z","shell.execute_reply.started":"2023-11-23T02:34:59.369196Z","shell.execute_reply":"2023-11-23T02:34:59.379646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['default'] == 1].shape","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.382316Z","iopub.execute_input":"2023-11-23T02:34:59.382751Z","iopub.status.idle":"2023-11-23T02:34:59.392189Z","shell.execute_reply.started":"2023-11-23T02:34:59.382720Z","shell.execute_reply":"2023-11-23T02:34:59.390978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qtd_total_novo, _ = df.shape\nqtd_adimplentes_novo, _ = df[df['default'] == 0].shape\nqtd_inadimplentes_novo, _ = df[df['default'] == 1].shape","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.393937Z","iopub.execute_input":"2023-11-23T02:34:59.394294Z","iopub.status.idle":"2023-11-23T02:34:59.407490Z","shell.execute_reply.started":"2023-11-23T02:34:59.394263Z","shell.execute_reply":"2023-11-23T02:34:59.406456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The proportion of active defaulters is {round(100 * qtd_adimplentes / qtd_total, 2)}%\")\nprint(f\"The new proportion of paying customers is {round(100 * qtd_adimplentes_novo / qtd_total_novo, 2)}%\")\nprint(f\"The proportion of defaulting customers is {round(100 * qtd_inadimplentes / qtd_total, 2)}%\")\nprint(f\"The new proportion of defaulting customers is {round(100 * qtd_inadimplentes_novo / qtd_total_novo, 2)}%\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.408934Z","iopub.execute_input":"2023-11-23T02:34:59.409994Z","iopub.status.idle":"2023-11-23T02:34:59.421154Z","shell.execute_reply.started":"2023-11-23T02:34:59.409952Z","shell.execute_reply":"2023-11-23T02:34:59.420079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Exploratory Analysis with Visualizations\n\nNow that the data is prepared, our next step involves creating various visualizations to uncover correlations between explanatory variables and the response variable. This will help us discern the factors that might contribute to a customer defaulting. Our approach involves comparing the entire customer base with subsets of compliant and non-compliant customers.\n\nTo kick off this analysis, we import the necessary visualization libraries and segment the dataset into compliant and non-compliant customer groups.","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.422547Z","iopub.execute_input":"2023-11-23T02:34:59.423155Z","iopub.status.idle":"2023-11-23T02:34:59.436036Z","shell.execute_reply.started":"2023-11-23T02:34:59.423123Z","shell.execute_reply":"2023-11-23T02:34:59.434580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_adimplente = df[df['default'] == 0]","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.437305Z","iopub.execute_input":"2023-11-23T02:34:59.437690Z","iopub.status.idle":"2023-11-23T02:34:59.448174Z","shell.execute_reply.started":"2023-11-23T02:34:59.437658Z","shell.execute_reply":"2023-11-23T02:34:59.446779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_inadimplente = df[df['default'] == 1]","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.449598Z","iopub.execute_input":"2023-11-23T02:34:59.450634Z","iopub.status.idle":"2023-11-23T02:34:59.463933Z","shell.execute_reply.started":"2023-11-23T02:34:59.450590Z","shell.execute_reply":"2023-11-23T02:34:59.462899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical Views\n\n*In this* section, we'll explore the relationship between the **default** response variable and categorical attributes through insightful visualizations.","metadata":{}},{"cell_type":"code","source":"df.select_dtypes('object').head(n=5)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:34:59.465332Z","iopub.execute_input":"2023-11-23T02:34:59.466412Z","iopub.status.idle":"2023-11-23T02:34:59.486317Z","shell.execute_reply.started":"2023-11-23T02:34:59.466371Z","shell.execute_reply":"2023-11-23T02:34:59.484584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* \"Escolaridade\"","metadata":{}},{"cell_type":"code","source":"coluna = 'escolaridade'\ntitulos = ['Escolaridade dos Clientes', 'Escolaridade dos Clientes Adimplentes', 'Escolaridade dos Clientes Inadimplentes']\n\n# Manually set colors for each category\ncolors = {'sem educacao formal': 'blue', 'ensino medio': 'orange', 'graduacao': 'green', 'mestrado': 'red', 'doutorado': 'purple'}\n\n# Sort unique values alphabetically\ncategories = sorted(df[coluna].unique())\n\n# Create a figure with three subplots\nfigura, eixos = plt.subplots(1, 3, figsize=(20, 5))\n\n# Use a loop to make a bar plot for each dataframe\nfor i in range(3):\n    # Choose the dataframe and title according to the index\n    if i == 0:\n        dataframe = df\n        titulo = titulos[0]\n    elif i == 1:\n        dataframe = df_adimplente\n        titulo = titulos[1]\n    else:\n        dataframe = df_inadimplente\n        titulo = titulos[2]\n\n    # Count the values in the column and make a bar plot\n    f = sns.countplot(x=coluna, data=dataframe, ax=eixos[i], palette=[colors[cat] for cat in categories])\n    f.set(title=titulo, xlabel=coluna.capitalize(), ylabel='Frequência Absoluta')\n    f.set_xticklabels(labels=categories, rotation=90)\n\n    # Set standard y-axis scale\n    f.set(ylim=(0, max(df[coluna].value_counts()) + 5))\n\n# Adjust the spacing between subplots\nfigura.tight_layout()\n\nfigura.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:35:41.914278Z","iopub.execute_input":"2023-11-23T02:35:41.914720Z","iopub.status.idle":"2023-11-23T02:35:43.174498Z","shell.execute_reply.started":"2023-11-23T02:35:41.914686Z","shell.execute_reply":"2023-11-23T02:35:43.173126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Salário Anual","metadata":{}},{"cell_type":"code","source":"# Manually set colors for each category\ncolors = {'120k+': 'purple', '40k - 60k': 'orange', '60k - 80k': 'green', '80k - 120k': 'red', 'menos que 40k': 'blue'}\n\n# Create a figure with three subplots\nfigura, eixos = plt.subplots(1, 3, figsize=(20, 5), sharex=True)\n\nmax_y = 0\n\nfor dataframe, titulo, eixo in zip([df, df_adimplente, df_inadimplente], titulos, eixos):\n\n    df_to_plot = dataframe[coluna].value_counts().to_frame().reset_index()\n    df_to_plot.columns = [coluna, 'frequencia_absoluta']\n\n    # Manually sort the categories in alphabetical order\n    categories = ['120k+', '40k - 60k', '60k - 80k', '80k - 120k', 'menos que 40k']\n\n    # Use hue to distinguish between dataframes\n    f = sns.barplot(x=coluna, y='frequencia_absoluta', data=df_to_plot, ax=eixo, hue='frequencia_absoluta', dodge=False,\n                    palette=[colors[cat] for cat in categories])\n    \n    # Set the title and x-axis label\n    f.set(title='Salarios dos clientes', xlabel='Salario Anual', ylabel='Frequência Absoluta')\n    f.set_xticklabels(labels=categories, rotation=90)\n\n    # Set the same y-axis limit for better comparison\n    max_y_f = f.get_ylim()[1]\n    max_y = max_y_f if max_y_f > max_y else max_y\n    f.set(ylim=(0, max_y))\n\nfigura.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:57:40.214960Z","iopub.execute_input":"2023-11-23T02:57:40.215540Z","iopub.status.idle":"2023-11-23T02:57:42.159561Z","shell.execute_reply.started":"2023-11-23T02:57:40.215491Z","shell.execute_reply":"2023-11-23T02:57:42.158034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical Views\n\nIn this section, we will delve into the correlation between the response variable **default** and the numeric attributes, shedding light on the quantitative aspects of our analysis.","metadata":{}},{"cell_type":"code","source":"df.drop(['id', 'default'], axis=1).select_dtypes('number').head(n=5)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:35:02.749386Z","iopub.execute_input":"2023-11-23T02:35:02.750294Z","iopub.status.idle":"2023-11-23T02:35:02.775498Z","shell.execute_reply.started":"2023-11-23T02:35:02.750254Z","shell.execute_reply":"2023-11-23T02:35:02.773800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Number of Transactions in the Last 12 Months","metadata":{}},{"cell_type":"code","source":"coluna = 'qtd_transacoes_12m'\ntitulos = ['Qtd. de Transações no Último Ano', 'Qtd. de Transações no Último Ano de Adimplentes', 'Qtd. de Transações no Último Ano de Inadimplentes']\n\nfigura, eixos = plt.subplots(1, 3, figsize=(20, 5), sharex=True)\n\nfor i, dataframe in enumerate([df, df_adimplente, df_inadimplente]):\n    ax = eixos[i]\n\n    # Use histplot for better representation of numerical data\n    f = sns.histplot(x=coluna, data=dataframe, stat='count', ax=ax, bins=20)\n\n    # Set plot labels and title\n    f.set(title=titulos[i], xlabel=coluna.capitalize(), ylabel='Frequência Absoluta')\n\n    # Set y-axis limit for consistency across subplots\n    f.set(ylim=(0, max_y))\n\n# Display the plot\nfigura.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:35:02.777287Z","iopub.execute_input":"2023-11-23T02:35:02.777765Z","iopub.status.idle":"2023-11-23T02:35:04.276235Z","shell.execute_reply.started":"2023-11-23T02:35:02.777724Z","shell.execute_reply":"2023-11-23T02:35:04.274969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Value of Transactions in the Last 12 Months","metadata":{}},{"cell_type":"code","source":"coluna = 'valor_transacoes_12m'\ntitulos = ['Valor das Transações no Último Ano', 'Valor das Transações no Último Ano de Adimplentes', 'Valor das Transações no Último Ano de Inadimplentes']\n\neixo = 0\nmax_y = 0\nfigura, eixos = plt.subplots(1,3, figsize=(20, 5), sharex=True)\n\nfor dataframe in [df, df_adimplente, df_inadimplente]:\n\n  f = sns.histplot(x=coluna, data=dataframe, stat='count', ax=eixos[eixo])\n  f.set(title=titulos[eixo], xlabel=coluna.capitalize(), ylabel='Frequência Absoluta')\n\n  _, max_y_f = f.get_ylim()\n  max_y = max_y_f if max_y_f > max_y else max_y\n  f.set(ylim=(0, max_y))\n\n  eixo += 1\n\nfigura.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:35:04.278133Z","iopub.execute_input":"2023-11-23T02:35:04.279831Z","iopub.status.idle":"2023-11-23T02:35:06.098107Z","shell.execute_reply.started":"2023-11-23T02:35:04.279770Z","shell.execute_reply":"2023-11-23T02:35:06.095439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Value of Transactions in the Last 12 Months x Number of Transactions in the Last 12 Months","metadata":{}},{"cell_type":"code","source":"# Define a custom color palette with distinct colors\ncustom_palette = {0: 'blue', 1: 'red'}\n\n# Use sns.scatterplot with the custom color palette\nf = sns.scatterplot(x='valor_transacoes_12m', y='qtd_transacoes_12m', data=df, hue='default', palette=custom_palette)\n\n# Set plot labels and title\nf.set(\n    title='Relação entre Valor e Quantidade de Transações no Último Ano',\n    xlabel='Valor das Transações no Último Ano',\n    ylabel='Quantidade das Transações no Último Ano'\n)\n\n# Improve legend placement for better visibility\nf.legend(loc='upper right')\n\n# Display the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:35:06.100489Z","iopub.execute_input":"2023-11-23T02:35:06.101014Z","iopub.status.idle":"2023-11-23T02:35:06.886698Z","shell.execute_reply.started":"2023-11-23T02:35:06.100966Z","shell.execute_reply":"2023-11-23T02:35:06.885360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Summary of insights generated\n\n- **Relationship between education and non-compliance**: There is a positive correlation between the level of education and the likelihood of being non-compliant, as shown by the higher proportion of non-compliant clients among those with doutorado e mestrado.\n\n- **Relationship between income and compliance**: There is a positive correlation between the annual salary and the likelihood of being defaulting, as shown by the higher frequency of defaulting clients among those with highest income brackets, and the lower frequency of defaulting clients among those with lowest income brackets.\n\n- **Income distribution of clients**: The overall client base has a skewed distribution of income, with most clients earning between $81k and $120k annually, and fewer clients earning above or below that range. The compliant clients follow a similar pattern, but with slightly lower frequencies across all income brackets.\n\n- **Transaction frequency and compliance**: There is a positive correlation between the quantity of transactions in the last year and the likelihood of being compliant, as shown by the higher frequency of compliant clients among those with 60-80 transactions, and the higher frequency of defaulting clients among those with fewer transactions (35-45).\n\n- **Transaction distribution of clients**: The overall client base has a normal distribution of transaction quantities, with most clients having 60-80 transactions in the last year. The compliant clients follow a similar pattern, but with a slightly higher frequency across all transaction ranges.\n\n- **Relationship between value and quantity of transactions and clients**: The most part of transactins are located in the lower end of the value scale and also represent the focus of the mass of the higher percentage of clients.\n\n- **Transaction types and characteristics**: There is an overlap region where both types of transactions (low value and high valur) are mixed, indicating a transitional zone where the characteristics of the transactions change.","metadata":{}}]}